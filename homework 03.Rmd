---
title: "Homework 04"
author: "Yue Li"
date: "3/6/2018"
output: html_document
---
#task 1
```{r load data}
library(cluster)


data.url = 'http://www.yurulin.com/class/spring2018_datamining/data/'
data = read.csv(sprintf("%s/stock_price.csv",data.url))
data[1:3,]
```

```{r task1.1}

#cor(data)
dim(data)
stock <- t(data)
stock[1:3,]
pcadata = prcomp(stock, scale=TRUE) 
#pcadata

datapc = predict(pcadata)
#datapc
screeplot(pcadata)
mtext(side=1, "Stock",  line=1, font=2)
plot(datapc[,1:2], type="n", xlim=c(-15,20))
text(x=datapc[,1], y=datapc[,2],labels=colnames(data))


plot(datapc[,3:4], type="n", xlim=c(-6,10))
text(x=datapc[,3], y=datapc[,4],labels=colnames(data))

biplot(pcadata)

plot(datapc[,1], type="n", xlim=c(0,35))
text(x=datapc[,1],labels=colnames(data))
```

```{r MDS}
set.seed(12345)

stock <- as.matrix(stock)


mult = stock %*% t(stock)
#mult
dim(mult)
sqrt(sum((mult[1, ] - mult[30, ]) ^ 2))

dist = dist(mult)
#dist

ex.mds <- cmdscale(dist)
#ex.mds
plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(stock))
```
```{r kmean 3}

#stock[1:3,]
scaled.data = apply(ex.mds,2,function(x) (x-mean(x)))
scaled.data
plot(ex.mds,cex=0.5,col="blue",main="Scaled Data")

set.seed(1234) ## fix the random seed to produce the same results 
grpStock = kmeans(scaled.data, centers=3, nstart=10)
grpStock
o=order(grpStock$cluster)
a <- data.frame(scaled.data[o],grpStock$cluster[o])
plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = grpStock$cluster+1)
```


```{r kmean 6}

grpStock = kmeans(scaled.data, centers=6, nstart=10)
grpStock
o=order(grpStock$cluster)
a <- data.frame(scaled.data[o],grpStock$cluster[o])
plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = grpStock$cluster+1)


```



```{r average 3}

scaled.data


stockagg = agnes(stock,diss=FALSE,metric="euclidian")
#plot(stockagg, which.plots=2)      #average
data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='average')
#data.dist
plot(hc)
hc1 = cutree(hc,k=3)
#hc1 <- as.matrix(hc1)
stock3 <- cbind(ex.mds,hc1)
stock3

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```

```{r average 6 }

scaled.data

data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='average')
#data.dist
plot(hc)
hc1 = cutree(hc,k=6)
stock3 <- cbind(ex.mds,hc1)

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```


```{r single 3 }


data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='single')
#data.dist
plot(hc)
hc1 = cutree(hc,k=3)
stock3 <- cbind(ex.mds,hc1)

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```


```{r single 6 }


data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='single')
#data.dist
plot(hc)
hc1 = cutree(hc,k=6)
stock3 <- cbind(ex.mds,hc1)

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```


```{r complete 3 }


data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='complete')
#data.dist
plot(hc)
hc1 = cutree(hc,k=3)
stock3 <- cbind(ex.mds,hc1)

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```


```{r complete 6 }


data.dist = dist(stock) ## use dist to obtain distance matrix
hc = hclust(data.dist,method='complete')
#data.dist
plot(hc)
hc1 = cutree(hc,k=6)
stock3 <- cbind(ex.mds,hc1)

plot(ex.mds, type = 'n')
text(ex.mds,labels=row.names(scaled.data),col = hc1+1)
```


#task 2 
```{r load data2}
library(car)
library('foreign') ## for loading dta files using read.dta

data.url = 'http://www.yurulin.com/class/spring2017_datamining/data/roll_call/sen113kh.dta'
data = read.dta(data.url,convert.factors = FALSE)
dim(data)

#data=sen113kh
data[1:3,1:10]

no.pres <- subset(data, state < 99)

dim(no.pres)
no.pres$party <- recode(no.pres$party,"100=1;200=2;else=0")   #recode party column: 1=dem and 2=repub

for (j in 1:105) {
  for(i in 10:666) {
    no.pres[j,i] = ifelse(no.pres[j,i] > 6, 0 , no.pres[j,i])
    no.pres[j,i] = ifelse(no.pres[j,i] > 0 & no.pres[j,i] < 4, 1 , no.pres[j,i])
    no.pres[j,i] = ifelse(no.pres[j,i] > 1, -1 , no.pres[j,i])
    
  }
}

```


```{r 2.1 MDS(party)}

vote.dist = dist(no.pres[,10:666])
#vote.dist
vote.mds <- cmdscale(vote.dist)
vote.mds <- cbind(vote.mds,no.pres$party)
vote.mds

plot(vote.mds,type="n")
text(vote.mds,labels=no.pres$party)

plot(vote.mds, type="n", xlab="", ylab="",main="MDS(party)")
points(x=vote.mds[,1], y=vote.mds[,2], pch=vote.mds[,3], col=vote.mds[,3]+1)

```


```{r 2.2 kmeans}
library(entropy)


set.seed(1234) ## fix the random seed to produce the same results 
grpVote = kmeans(vote.mds, centers=2, nstart=10)
grpVote
o=order(grpVote$cluster)
a <- data.frame(vote.mds[o],grpVote$cluster[o])
plot(vote.mds, type = 'n',main="kmeans")
points(vote.mds,pch=grpVote$cluster, col=grpVote$cluster+1)

plot(vote.mds, type="n", xlab="", ylab="",main="MDS(party)")
points(x=vote.mds[,1], y=vote.mds[,2], pch=vote.mds[,3], col=vote.mds[,3]+1)    #red circle are democrats and green rectangulars are republicans

km.en <- entropy(grpVote$cluster,vote.mds[,3])
km.en
km.pu <- sum(apply(table(vote.mds[,3], grpVote$cluster), 2, max)) / length(hc1)
km.pu

```
There are three points assigned to the wrong cluster in k-means.(three points in the middle of this graph)



```{r single }

vote.dist = dist(no.pres[,10:666]) ## use dist to obtain distance matrix
hc = hclust(vote.dist,method='single')
#data.dist
plot(hc)
hc1 = cutree(hc,k=2)
hc1 <- recode(hc1,"1=2;2=1;else=0")
plot(vote.mds, type = 'n', main="single")
points(vote.mds,pch=hc1,col = hc1+1)

plot(vote.mds, type="n", xlab="", ylab="",main="MDS(party)")
points(x=vote.mds[,1], y=vote.mds[,2], pch=vote.mds[,3], col=vote.mds[,3]+1)

hcs.en <- entropy(hc1,vote.mds[,3])
hcs.en
hcs.pu <- sum(apply(table(vote.mds[,3], hc1), 2, max)) / length(hc1)
hcs.pu
```
Assigned all Democrat to Republican and assigned two Republican to Democrat in single link clustering.



```{r complete }
library("entropy")
library(car)
  
vote.dist = dist(no.pres[,10:666]) ## use dist to obtain distance matrix
hc = hclust(vote.dist,method='complete')
#data.dist
plot(hc)
hc1 = cutree(hc,k=2)
hc1 <- recode(hc1,"1=2;2=1;else=0")

plot(vote.mds, type = 'n', main="complete")
points(vote.mds,pch=hc1,col = hc1+1)


plot(vote.mds, type="n", xlab="", ylab="",main="MDS(party)")
points(x=vote.mds[,1], y=vote.mds[,2], pch=vote.mds[,3], col=vote.mds[,3]+1)
hcc.en <- entropy(hc1,vote.mds[,3])
hcc.en
hcc.pu <- sum(apply(table(vote.mds[,3], hc1), 2, max)) / length(hc1)
hcc.pu

```
Three republicans are assigned to democrats in the complete link clustering.



```{r average }


vote.dist = dist(no.pres[,10:666]) ## use dist to obtain distance matrix
hc = hclust(vote.dist,method='average')
#data.dist
plot(hc)
hc1 = cutree(hc,k=2)
hc1 <- recode(hc1,"1=2;2=1;else=0")

plot(vote.mds, type = 'n', main="average")
points(vote.mds,pch=hc1,col = hc1+1)


plot(vote.mds, type="n", xlab="", ylab="",main="MDS(party)")
points(x=vote.mds[,1], y=vote.mds[,2], pch=vote.mds[,3], col=vote.mds[,3]+1)

hca.en <- entropy(hc1,vote.mds[,3])
hca.en
hca.pu <- sum(apply(table(vote.mds[,3], hc1), 2, max)) / length(hc1)
hca.pu
```

Three republicans are assigned to democrats in the complete link clustering.



```{r table}
Columnname=c("Entropy","Purity")
Rowname = c("k.mean","HC.single","HC.complete","HC.average")
Entropy <- c(km.en,hcs.en,hcc.en,hca.en)
Purity <- c(km.pu,hcs.pu,hcc.pu,hca.pu)


matrix(c(Entropy,Purity), nrow = 2, ncol = 4, byrow = TRUE, dimnames = list(Columnname, Rowname))

```

I would select hierarchical clustering with complete link and average link. Because these two methods generate highest purity and the lowest entropy.
