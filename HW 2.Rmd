---
title: "Homework 2"
author: "Yue Li"
date: "2/16/2018"
output: html_document
---


```{r do.classification}
library(MASS) # for the example dataset 
library(plyr) # for recoding data
library(ROCR) # for plotting roc
library(e1071) # for NB and SVM
library(rpart) # for decision tree
library(ada) # for adaboost
library(class)
library(car)



set.seed(123) # set the seed so you can get exactly the same results whenever you run the code

do.classification <- function(train.set, test.set, cl.name, verbose=F) {
  ## note: to plot ROC later, we want the raw probabilities, not binary decisions
  switch(cl.name, 
         #variant for KNN, k=1
         knn1 = { # here we test k=3; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 1, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$y)))
           prob
         },
         knn3 = { # here we test k=3; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 3, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$y)))
           prob
         },
         #variant for KNN, k=5
         knn5 = { # here we test k=3; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 5, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$y)))
           prob
         },
         lr = { # logistic regression
           model = glm(y~., family=binomial, data=train.set)
           if (verbose) {
             print(summary(model))             
           }
           prob = predict(model, newdata=test.set, type="response") 
           #print(cbind(prob,as.character(test.set$y)))
           prob
         },
         nb = {
           model = naiveBayes(y~., data=train.set)
           prob = predict(model, newdata=test.set, type="raw") 
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         dtree = {
           model = rpart(y~., data=train.set)
           if (verbose) {
             print(summary(model)) # detailed summary of splits
             printcp(model) # print the cross-validation results
             plotcp(model) # visualize the cross-validation results
             ## plot the tree
             plot(model, uniform=TRUE, main="Classification Tree")
             text(model, use.n=TRUE, all=TRUE, cex=.8)
           }           
           prob = predict(model, newdata=test.set)
           
           if (0) { # here we use the default tree, 
             ## you should evaluate different size of tree
             ## prune the tree 
             pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
             prob = predict(pfit, newdata=test.set)
             ## plot the pruned tree 
             plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
             text(pfit, use.n=TRUE, all=TRUE, cex=.8)             
           }
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         svm = {
           model = svm(y~., data=train.set, probability=T)
           if (0) { # fine-tune the model with different kernel and parameters
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, kernel="radial", gamma = 10^(-6:-1), cost = 10^(-1:1))
             #print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=T, 
                         kernel="radial", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=T)
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.character(test.set$y)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         #variant for SVM, kernel = polynomial
         svm2 = {
           model = svm(y~., data=train.set, probability=T)
           if (0) { # fine-tune the model with different kernel and parameters
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, kernel="polynomial", gamma = 10^(-6:-1), cost = 10^(-1:1))
             #print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=T, 
                         kernel="polynomial", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=T)
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.character(test.set$y)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         #variant for SVM, kernel = sigmoid
          svm3 = {
           model = svm(y~., data=train.set, probability=T)
           if (0) { # fine-tune the model with different kernel and parameters
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, kernel="sigmoid", gamma = 10^(-6:-1), cost = 10^(-1:1))
             #print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=T, 
                         kernel="sigmoid", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=T)
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.character(test.set$y)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         ada = {
           model = ada(y~., data = train.set)
           prob = predict(model, newdata=test.set, type='probs')
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob)
           prob
         }
  ) 
}

```



```{r pre.test}

pre.test <- function(dataset, cl.name, r=0.6, prob.cutoff=0.5) {
  ## Let's use 60% random sample as training and remaining as testing
  ## by default use 0.5 as cut-off
  n.obs <- nrow(dataset) # no. of observations in dataset
  n.train = floor(n.obs*r)
  train.idx = sample(1:n.obs,n.train)
  train.idx
  train.set = dataset[train.idx,]
  test.set = dataset[-train.idx,]
  cat('pre-test',cl.name,':',
      '#training:',nrow(train.set),
      '#testing',nrow(test.set),'\n')
  prob = do.classification(train.set, test.set, cl.name)
  # prob is an array of probabilities for cases being positive
  
  ## get confusion matrix
  predicted = as.numeric(prob > prob.cutoff)
  actual = test.set$y
  confusion.matrix = table(actual,factor(predicted,levels=c(0,1)))
  error = (confusion.matrix[1,2]+confusion.matrix[2,1]) / nrow(test.set)  
  cat('error rate:',error,'\n')
  # you may compute other measures based on confusion.matrix
  # @see handout03 p.32-36
  
  ## plot ROC
  result = data.frame(prob,actual)
  pred = prediction(result$prob,result$actual)
  perf = performance(pred, "tpr","fpr")
  plot(perf)    
}

```



```{r k-fold}

k.fold.cv <- function(dataset, cl.name, k.fold=10, prob.cutoff=0.5) {
  ## default: 10-fold CV, cut-off 0.5 
  n.obs <- nrow(dataset) # no. of observations 
  s = sample(n.obs)
  errors = dim(k.fold)
  probs = NULL
  actuals = NULL
  for (k in 1:k.fold) {
    test.idx = which(s %% k.fold == (k-1) ) # use modular operator
    train.set = dataset[-test.idx,]
    test.set = dataset[test.idx,]
    #cat(k.fold,'-fold CV run',k,cl.name,':','#training:',nrow(train.set), '#testing',nrow(test.set),'\n')
    prob = do.classification(train.set, test.set, cl.name)
    predicted = as.numeric(prob > prob.cutoff)
    actual = test.set$y
    confusion.matrix = table(actual,factor(predicted,levels=c(0,1)))
    confusion.matrix
    error = (confusion.matrix[1,2]+confusion.matrix[2,1]) / nrow(test.set)  
    errors[k] = error
    #cat('\t\terror=',error,'\n')
    probs = c(probs,prob)
    actuals = c(actuals,actual)
    ## you may compute other measures and store them in arrays
  }
  avg.error = mean(errors)
  #cat(k.fold,'-fold CV results:','avg error=',avg.error,'\n')
  
  ## plot ROC
  result = data.frame(probs,actuals)
  
  pred <- prediction(result$probs,result$actuals)
  pred
  #set perf as a superassignment, so that we can use it out of the function
  perf <<- performance(pred, "tpr","fpr")
  #plot(perf,main=cl.name) 
  
  ## get other measures by using 'performance'
  get.measure <- function(pred, measure.name='auc') {
    perf = performance(pred,measure.name)
    m <- unlist(slot(perf, "y.values"))
#     print(slot(perf, "x.values"))
#     print(slot(perf, "y.values"))
    m
  }
  err = mean(get.measure(pred, 'err'))
  #accuracy = mean(1-err)
  precision = mean(get.measure(pred, 'prec'),na.rm=T)
  recall = mean(get.measure(pred, 'rec'),na.rm=T)
  fscore = mean(get.measure(pred, 'f'),na.rm=T)
  #cat('error=',err,'precision=',precision,'recall=',recall,'f-score',fscore,'\n')
  auc = get.measure(pred, 'auc')
  #cat('auc=',auc,'\n')
  op<<-c(err,precision,recall,fscore,auc)
}
```

```{r my.classification}
my.classifier <- function(dataset, cl.name='knn', do.cv=F) {
  n.obs <- nrow(dataset) # no. of observations in dataset
  n.cols <- ncol(dataset) # no. of predictors
  cat('my dataset:',
      n.obs,'observations',
      n.cols-1,'predictors','\n')
  print(dataset[1:3,])
  cat('label (y) distribution:')
  print(table(dataset$Total))
  
  pre.test(dataset, cl.name)
  if (do.cv) k.fold.cv(dataset, cl.name)
}
```




```{r main(table)}
### main ###

pokemon <- read.csv("http://www.yurulin.com/class/spring2018_datamining/data/pokemon/pokemon.csv")
#dim(pokemon)
dataset <- pokemon[,3:13]
#recode Type.1 , Type.2 , Legendary and Total into numeric data
dataset$Type.1 = recode(dataset$Type.1, "'Grass' = 1; 'Fire'=2 ; 'Water'=3; 'Poison'=4; 'Flying'=5 ; 'Dragon'=6;'Bug'=7; 'Ground'=8; 'Electric'=9; 'Normal'=10; 'Fairy'=11; 'Fighting'=12; 'Psychic'=13; 'Rock'=14;'Steel'=15;'Ghost'=16;'Dark'=17;'Ice'=18")
dataset$Type.2 = recode(dataset$Type.2, "'Grass' = 1; 'Fire'=2 ; 'Water'=3; 'Poison'=4; 'Flying'=5 ; 'Dragon'=6;'Bug'=7; 'Ground'=8; 'Electric'=9; 'Normal'=10; 'Fairy'=11; 'Fighting'=12; 'Psychic'=13; 'Rock'=14;'Steel'=15;'Ghost'=16;'Dark'=17;'Ice'=18;'NO'=0")

dataset$Legendary = recode(dataset$Legendary, "'TRUE'=1;else=0")
dataset$Total = recode(dataset$Total, "'>500'=1;else=0")
#change column name 'Total' -> y
dataset$y = dataset$Total
dataset<-dataset[,-11]
dataset[1:3,]



k.fold.cv(dataset, cl.name='knn1', k.fold=10, prob.cutoff=0.5)
KNN1 <- op
roc1 <- perf
#perf <- performance(pred, "tpr","fpr")
#plot(perf)


k.fold.cv(dataset, cl.name='knn3', k.fold=10, prob.cutoff=0.5)
KNN3 <- op
roc2 <- perf
k.fold.cv(dataset, cl.name='knn5', k.fold=10, prob.cutoff=0.5)
KNN5 <-op
roc3 <- perf
k.fold.cv(dataset, cl.name='lr', k.fold=10, prob.cutoff=0.5)
LR <- op
roc4=perf
k.fold.cv(dataset, cl.name='dtree', k.fold=10, prob.cutoff=0.5)
DTree <- op
roc5 <- perf
k.fold.cv(dataset, cl.name='nb', k.fold=10, prob.cutoff=0.5)
NB <-op
roc6 <- perf
k.fold.cv(dataset, cl.name='svm', k.fold=10, prob.cutoff=0.5)
SVM <- op
roc7 <- perf
#SVM2: kernel = polynomial
k.fold.cv(dataset, cl.name='svm2', k.fold=10, prob.cutoff=0.5)
SVM2 <- op
roc8 <- perf
#SVM3: kernel = sigmoid
k.fold.cv(dataset, cl.name='svm3', k.fold=10, prob.cutoff=0.5)
SVM3 <- op
roc9 <- perf


#print the result table:
Columnname = c("error", "precision","recall","fscore","AUC")
Rowname = c("KNN1","KNN3", "KNN5","LR","DTree","NB","SVM","SVM2","SVM3")    
SUMMARY = matrix(c(KNN1,KNN3,KNN5,LR,DTree,NB,SVM,SVM2,SVM3), nrow = 5, ncol = 9, byrow = TRUE, dimnames = list(Columnname, Rowname))
cat('\n')
SUMMARY
```


```{r bar charts}
#bar charts
fs<-SUMMARY[4,]
auc<-SUMMARY[5,]
x <- c("knn1","knn3","knn5","lr","dtree","nb","svm","svm.poly","svm.sig")
#plot_ly(data, x = ~x, y = ~fs, type = 'bar', name = 'FSCORE', marker = list(color = 'rgb(49,130,189)')) %>%
 # add_trace(y = ~auc, name = 'AUC', marker = list(color = 'rgb(204,204,204)')) %>%
 # layout(xaxis = list(title = "", tickangle = -45), yaxis = list(title = ""), margin = list(b = 100), barmode = 'group')
#plot_ly(data, x = ~x, y = ~fs, type = 'bar',  marker = list(color = 'rgb(49,130,189)')) %>%
 # layout(xaxis = list(title = "", tickangle = -45), yaxis = list(title = ""), margin = list(b = 100), barmode = 'group')
#plot_ly(data, x = ~x, y = ~auc, type = 'bar', marker = list(color = 'rgb(49,130,189)')) %>%
 # layout(xaxis = list(title = "", tickangle = -45), yaxis = list(title = ""), margin = list(b = 100), barmode = 'group')
barplot(fs, main="FSCORE", xlab="technique",col="lightsteelblue")
barplot(auc, main="AUC", xlab="technique",col="slategray3")
```


```{r ROC}
#plot ROC curve in one figure#
#since the auc of knn3(0.815) is higher than knn1 and knn5, plot knn3 
plot(roc2, col="red")
#roc4 is the linear regression curve
plot(roc4, add = TRUE,col="green")
#roc5 is the  decision tree curve
plot(roc5, add = TRUE,col="blue")
#roc6 is the naive bayesian curve
plot(roc6, add = TRUE,col="brown")
#since the auc of cvm3(0.995) is higher than cvm1 and cvm2, plot knn3 
plot(roc9, add = TRUE,col="black")
legend("topright", legend=c("knn3","linear regression","decision tree","naive bayesian","svm.sigmoid"),
       col=c("red", "green","blue","brown","black"), lty=1, cex=0.8,
       title="Line types", text.font=4, bg="cornsilk")
```

Summary:
According to the result table, the auc of linear regression and SVM(sigmoid) are the highest. However, their f-scores are extremely low. On the other hand, the performances of KNN5, Naive Bayesian and SVM2 are generally good. 
According to the ROC graph, the best three models are linear regression, SVM3 and naive bayesian. 

Above all, I think the best model is naive bayesian because it perform good overall. However, if we don't consider f-measure, the linear regression is the best model.